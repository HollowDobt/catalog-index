"""
=============================
|src/domains/orchestrator.py|
=============================

# Intelligent Research Agent
# Advanced AI Agent with State Planning and Adaptive Execution
"""

from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional, Tuple, Callable
from concurrent.futures import ThreadPoolExecutor, as_completed
from infrastructure import *
from domains import *
from enum import Enum, auto

import time
import threading
import queue
import json
import uuid
import math
import re
from collections import defaultdict


class AgentState(Enum):
    """
    Agent execution states
    """

    INITIALIZING = (
        auto()
    )  # Initialize the agent, load resources, and prepare the environment.
    ANALYZING_QUERY = (
        auto()
    )  # Analyze the query or task entered by the user to understand the intent.
    PLANNING_SEARCH = (
        auto()
    )  # Develop a search plan, such as selecting keywords, data sources, and API call methods.
    EXECUTING_SEARCH = (
        auto()
    )  # Perform search operations (calling an API, querying a database, etc.).
    PROCESSING_RESULTS = (
        auto()
    )  # Perform preliminary processing on search results (deduplication, screening, and structuring).
    EVALUATING_RESULTS = (
        auto()
    )  # Evaluate the quality of the results and determine whether they meet the requirements.
    REFINING_STRATEGY = (
        auto()
    )  # If the results are not satisfactory, adjust the strategy (change keywords, change data sources, etc.).
    SYNTHESIZING = (
        auto()
    )  # Organize and integrate the final results into output content.
    COMPLETED = auto()  # Mission accomplished.
    ERROR = (
        auto()
    )  # An error occurred (such as network anomaly, data parsing failure, etc.).


class ActionType(Enum):
    """
    Types of actions the agent can take
    """

    QUERY_ANALYSIS = auto()  # Parse the input query, identify intent, and split tasks.
    KEYWORD_GENERATION = auto()  # Generate search keywords or query expressions.
    SEARCH_EXECUTION = auto()  # Perform a search directly
    RESULT_PROCESSING = auto()  # Parse, clean, and format the raw results.
    STRATEGY_REFINEMENT = (
        auto()
    )  # Adjust search/processing strategies to achieve better results.
    SYNTHESIS = auto()  # Synthesize the processed information into a final answer.


@dataclass
class ExecutionContext:
    """
    Context information for agent decisions
    """

    current_state: AgentState
    search_attempts: int
    total_papers_found: int
    processed_papers: int
    successful_analyses: int
    failed_analyses: int
    current_keywords: str
    user_query: str
    execution_history: List[Dict[str, Any]] = field(default_factory=list)
    search_results: List[Dict[str, Any]] = field(default_factory=list)
    analysis_results: List[str] = field(default_factory=list)

    # Log component function
    def add_execution_record(self, action: ActionType, details: Dict[str, Any]):
        """
        Record an execution step
        """
        self.execution_history.append(
            {
                "timestamp": time.time(),
                "action": action.name,
                "state": self.current_state.name,
                "details": details,
            }
        )


@dataclass
class ArxivRateLimiter:
    """
    ArXiv API rate limiter - strictly adheres to official documentation requirements
    """

    min_interval: int
    last_request_time = 0
    lock = threading.Lock()

    def wait_if_needed(self):
        """
        Ensure that the request interval >= 3 seconds
        """
        with self.lock:

            current_time = time.time()
            time_since_last = current_time - self.last_request_time

            if time_since_last < self.min_interval:
                sleep_time = self.min_interval - time_since_last
                time.sleep(sleep_time)

            self.last_request_time = time.time()


# Global rate limiter
arxiv_rate_limiter = ArxivRateLimiter(min_interval=3)


class IntelligentResearchAgent:
    """
    Advanced AI Agent with state-based planning and adaptive execution
    """

    def __init__(self, config: Dict[str, Any]):
        """
        Minimum parameters provided: front-end interactive interface,
        original paper processing LLM, API generator, embedding model
        """
        self.config = config

        # State Log -> context
        self.context = ExecutionContext(
            current_state=AgentState.INITIALIZING,
            search_attempts=0,
            total_papers_found=0,
            processed_papers=0,
            successful_analyses=0,
            failed_analyses=0,
            current_keywords="",
            user_query="",
        )

        # Initialize clients
        self.interface = IOStream.create(config["interface"])
        self.llm_query_processor = LLMClient.create(
            config["raw_message_process_llm"],
            model=config["raw_message_process_llm_model"],
        )
        self.llm_api_generator = LLMClient.create(
            config["api_generate_llm"], model=config["api_generate_llm_model"]
        )
        self.llm_embedding = LLMClient.create(
            config["embedding_llm"], model=config["embedding_llm_model"]
        )

        # Initialize tools
        self.api_rag = AcademicDBRAG.create("arxiv", LLM_client=self.llm_api_generator)
        self.metadata_client = AcademicDBClient.create("arxiv")
        self.memory = Mem0Client()
        self.pdf_parser = PDFToMarkdownConverter()
        self.article_processor = ArticleStructuring(
            llm=config["raw_article_process_llm"],
            llm_model=config["raw_message_process_llm_model"],
        )

        # Thread management
        self.max_workers = config.get("max_workers_llm", 8)
        self.max_search_retries = config.get("max_search_retries", 3)
        self.result_queue = queue.Queue()

        # Agent decision system
        # State Mapping Table: From state to function
        self.state_handlers = {
            AgentState.INITIALIZING: self._handle_initialization,
            AgentState.ANALYZING_QUERY: self._handle_query_analysis,
            AgentState.PLANNING_SEARCH: self._handle_search_planning,
            AgentState.EXECUTING_SEARCH: self._handle_search_execution,
            AgentState.PROCESSING_RESULTS: self._handle_result_processing,
            AgentState.EVALUATING_RESULTS: self._handle_result_evaluation,
            AgentState.REFINING_STRATEGY: self._handle_strategy_refinement,
            AgentState.SYNTHESIZING: self._handle_synthesis,
        }

    def _transition_state(
        self, new_state: AgentState, context_data: Optional[Dict[str, Any]] = None
    ):
        """
        Handle state transitions with logging
        """
        # Record the current status and then update the current status for the next step
        old_state = self.context.current_state
        self.context.current_state = new_state

        # Prepare log data. If context_data is None, use None
        transition_details = {
            "from_state": getattr(old_state, "name", None),
            "to_state": getattr(old_state, "name", None),
            "context_data": context_data,
        }

        print(f"ğŸ”„ Agent State: {old_state.name} â†’ {new_state.name}")
        self.context.add_execution_record(ActionType.QUERY_ANALYSIS, transition_details)

    # Search Key-words Enhance Need Assess
    def _evaluate_search_quality(self) -> Dict[str, Any]:
        """
        Evaluate the quality of current search results
        """
        # Storage of Assessment Content
        evaluation = {
            "papers_found": self.context.total_papers_found,
            "success_rate": (
                self.context.successful_analyses / max(1, self.context.processed_papers)
            ),
            "search_efficiency": self.context.total_papers_found
            / max(1, self.context.search_attempts),
            "needs_refinement": False,
            "suggested_action": "continue",
        }

        # Decision logic based on results

        # If a search fails, must search again.
        if self.context.total_papers_found == 0:
            evaluation["needs_refinement"] = True
            evaluation["suggested_action"] = "expand_keywords"

        # If the percentage of successfully parsed papers is too low, must search again.
        elif evaluation["success_rate"] < 0.3:
            evaluation["needs_refinement"] = True
            evaluation["suggested_action"] = "refine_keywords"

        # If the search returns too few results and the number of visits allows, must search again
        elif (
            self.context.total_papers_found < 3
            and self.context.search_attempts < self.max_search_retries
        ):
            evaluation["needs_refinement"] = True
            evaluation["suggested_action"] = "broaden_search"

        return evaluation

    # Search Key-words Enhance Generate
    def _generate_adaptive_keywords(self, evaluation: Dict[str, Any]) -> str:
        """
        Generate adaptive keywords based on search evaluation
        """
        prompt_context = f"""
### åŸå§‹æŸ¥è¯¢: {self.context.user_query}
### å½“å‰å…³é”®è¯: {self.context.current_keywords}
### æœç´¢å°è¯•æ¬¡æ•°: {self.context.search_attempts}
### æ‰¾åˆ°è®ºæ–‡æ•°é‡: {self.context.total_papers_found}
### å¤„ç†æˆåŠŸç‡: {evaluation['success_rate']:.2f}
### å»ºè®®è¡ŒåŠ¨: {evaluation['suggested_action']}
        
## æ‰§è¡Œå†å²æ‘˜è¦:
{self._summarize_execution_history()}
        
## åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œç”Ÿæˆä¼˜åŒ–çš„æœç´¢å…³é”®è¯ç­–ç•¥ï¼š
    1. å¦‚æœæ²¡æ‰¾åˆ°è®ºæ–‡ï¼Œæ‰©å±•æœç´¢èŒƒå›´ï¼Œä½¿ç”¨æ›´é€šç”¨æœ¯è¯­
    2. å¦‚æœæˆåŠŸç‡ä½ï¼Œç²¾ç‚¼å…³é”®è¯ï¼Œæé«˜ç›¸å…³æ€§
    3. å¦‚æœè®ºæ–‡æ•°é‡å°‘ï¼Œå°è¯•ç›¸å…³é¢†åŸŸæˆ–åŒä¹‰è¯
    4. ç»“åˆæ‰§è¡Œå†å²é¿å…é‡å¤æ— æ•ˆæœç´¢
        
## è¾“å‡ºæ ¼å¼ï¼šä»…è¾“å‡ºå…³é”®è¯ï¼Œç”¨å¥å·åˆ†å‰²ï¼Œä¸è¦å…¶ä»–å†…å®¹
"""
        message = [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æœç´¢ç­–ç•¥ä¼˜åŒ–å™¨ï¼Œæ ¹æ®æœç´¢å†å²å’Œç»“æœè´¨é‡ç”Ÿæˆæœ€ä¼˜å…³é”®è¯ç»„åˆã€‚",
            },
            {"role": "user", "content": prompt_context},
        ]

        response = self.llm_query_processor.chat_completion(
            messages=message, temperature=0.3
        )

        return response["choices"][0]["message"]["content"]

    # Return the summary of the last few records in the log
    def _summarize_execution_history(self) -> str:
        """
        Summarize execution history for context
        """
        if not self.context.execution_history:
            return "æ— æ‰§è¡Œå†å²"

        recent_actions = self.context.execution_history[
            -min(4, len(self.context.execution_history)) :
        ]  # Last 4 actions
        summary = []
        for action in recent_actions:
            summary.append(
                f"- {action['action']}: {action.get('details', {}).get('summary', 'æ‰§è¡Œå®Œæˆ')}"
            )

        return "\n".join(summary)

    # Filter invalid information to reduce token consumption
    def _filter_invalid_content(self, content: str) -> str:
        """
        Filter out invalid or meaningless content from analysis results

        params
        ------
        content: Original find_connect return value

        return
        ------
        Pure valid information after filtering the original value
        """
        if not content or not isinstance(content, str):
            return ""

        # Define patterns for invalid content
        invalid_patterns = [
            # Chinese
            r"æ²¡æœ‰?æ‰¾åˆ°.*?å…³è”",
            r"æœªèƒ½?æ‰¾åˆ°.*?è¿æ¥",
            r"æ— æ³•.*?å»ºç«‹è”ç³»",
            r"ç¼ºä¹.*?ç›¸å…³æ€§",
            r"ä¸å­˜åœ¨.*?ç›´æ¥å…³ç³»",
            r"æ— ç›¸å…³.*?ä¿¡æ¯",
            r"æ— æ³•.*?ç¡®å®šå…³è”",
            r"æœªå‘ç°.*?è”ç³»",
            r"æŠ±æ­‰.*?æ²¡æœ‰æ‰¾åˆ°",
            r"å¾ˆæŠ±æ­‰.*?æ— æ³•",
            r"å¯¹ä¸èµ·.*?æ‰¾ä¸åˆ°",
            r"æ²¡æœ‰ç›¸å…³.*?å†…å®¹"
            # English
            r"not\s+found.*?(connection|link|relation|association)",
            r"unable\s+to\s+find.*?(connection|link|relation|association)",
            r"cannot\s+(establish|create|make).*?(connection|link|relation|association)",
            r"lack(s)?\s+.*?(relevance|relevancy|relation|association)",
            r"no\s+.*?(direct\s+relation|direct\s+link|direct\s+connection)",
            r"no\s+related.*?(information|content|data)",
            r"cannot\s+determine.*?(relation|association|connection)",
            r"(not\s+found|did\s+not\s+find).*(contact|connection|relation|link)",
            r"sorry.*?(no|not\s+found|unable)",
            r"apologies.*?(no|not\s+found|unable)",
            r"no\s+related.*?(content|information|data)",
        ]

        # Check if content contains too many invalid patterns
        invalid_count = 0
        for pattern in invalid_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                invalid_count += 1

        # If more than half the content is invalid patterns, filter it out
        total_sentences = len(re.split(r"[ã€‚ï¼ï¼Ÿ.!?\n]", content))
        if invalid_count > total_sentences * 0.5:  # 50% threshold
            return ""

        # Remove specific invalid sentences but keep the rest
        filtered_content = content
        for pattern in invalid_patterns:
            filtered_content = re.sub(
                pattern + r"[ã€‚ï¼ï¼Ÿ]*", "", filtered_content, flags=re.IGNORECASE
            )

        # Clean up extra whitespace
        filtered_content = re.sub(r"\n\s*\n", "\n", filtered_content.strip())

        # Return empty if too short after filtering
        if len(filtered_content.strip()) < 50:
            return ""

        return filtered_content.strip()

    # Let the AI merge the results of the two diffenret find_connect calls
    def _merge_two_contents(
        self, content1: str, content2: str, max_tokens: int, level: int
    ) -> str:
        """
        Merge two content pieces using LLM with specified token limit

        params
        ------
        content1 & content2: Two result segments that need to be merged
        max_tokens: The maximum number of tokens allowed to be consumed
        level:

        return
        ------

        """

        # If both are empty, return empty
        if not content1 and not content2:
            return ""

        # If either content is empty after filtering, return the other
        if not content1:
            return content2
        if not content2:
            return content1

        system_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯ä¿¡æ¯æ•´åˆä¸“å®¶ã€‚æ“…é•¿å°†å¤šä¸ªç ”ç©¶å†…å®¹åˆå¹¶ä¸ºç»“æ„åŒ–ã€é€»è¾‘æ¸…æ™°çš„ç»¼åˆæŠ¥å‘Šã€‚è¯·å°†ç”¨æˆ·æä¾›çš„ä¸¤æ®µç ”ç©¶å†…å®¹è¿›è¡Œæ™ºèƒ½åˆå¹¶ï¼Œè¦æ±‚ï¼š

1. **ä¿æŒä¿¡æ¯å®Œæ•´æ€§**ï¼šä¸ä¸¢å¤±é‡è¦çš„ç ”ç©¶å‘ç°å’Œæ ¸å¿ƒè§‚ç‚¹
2. **æ¶ˆé™¤å†—ä½™**ï¼šåˆå¹¶é‡å¤ä¿¡æ¯ï¼Œé¿å…ä¸å¿…è¦çš„é‡å¤
3. **é€»è¾‘æ•´ç†**ï¼šæŒ‰ç…§é€»è¾‘å…³ç³»é‡æ–°ç»„ç»‡å†…å®¹ç»“æ„
4. **è¯­è¨€ä¼˜åŒ–**ï¼šç¡®ä¿åˆå¹¶åçš„å†…å®¹è¯­è¨€æµç•…ã€æ¡ç†æ¸…æ™°
5. **çªå‡ºå…³è”**ï¼šå¼ºè°ƒå†…å®¹é—´çš„å…³è”æ€§å’Œäº’è¡¥æ€§
6. **æ§åˆ¶é•¿åº¦**ï¼šåˆå¹¶åçš„å†…å®¹åº”æ§åˆ¶åœ¨{max_tokens}ä¸ªtokenä»¥å†…
"""

        merge_prompt = f"""
## ç”¨æˆ·åŸå§‹æŸ¥è¯¢
{self.context.user_query}

## å†…å®¹A
{content1}

## å†…å®¹B  
{content2}

## åˆå¹¶è¦æ±‚
- å›´ç»•ç”¨æˆ·æŸ¥è¯¢è¿›è¡Œå†…å®¹æ•´åˆ
- çªå‡ºä¸¤ä¸ªå†…å®¹çš„äº’è¡¥æ€§å’Œå…³è”æ€§
- å»é™¤å†—ä½™ä¿¡æ¯ï¼Œä¿ç•™æ ¸å¿ƒè§‚ç‚¹
- ç¡®ä¿åˆå¹¶åå†…å®¹é€»è¾‘æ¸…æ™°ã€ç»“æ„å®Œæ•´
- è¾“å‡ºç®€æ´ä¸”ä¿¡æ¯å¯†åº¦é«˜çš„æ•´åˆç»“æœ

è¯·ç›´æ¥è¾“å‡ºåˆå¹¶åçš„å†…å®¹ï¼Œä¸è¦åŒ…å«ä»»ä½•è¯´æ˜æ–‡å­—ï¼š
"""

        try:
            message = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": merge_prompt},
            ]

            response = self.llm_query_processor.chat_completion(
                messages=message, temperature=0.3, max_tokens=max_tokens
            )

            merged_content = response["choices"][0]["message"]["content"].strip()

            # Final filtering of the merged content
            final_content = self._filter_invalid_content(merged_content)
            return final_content if final_content else (content1 or content2)

        except Exception as exc:
            print(f"  âš ï¸ åˆå¹¶å¤±è´¥ (çº§åˆ« {level}): {exc}")
            # Fallback: simple concatenation with filtering
            fallback = f"{content1}\n\n{content2}"
            return self._filter_invalid_content(fallback) or (content1 or content2)

    # The final step is to combine multiple results into one
    def _intelligent_synthesis_merge(self, results: List[str]) -> str:
        """
        Use binary tree merging with thread pool for intelligent content synthesis
        """
        if not results:
            return ""

        # Filter out invalid results first
        valid_results: List[str] = []
        for result in results:
            filtered_result = self._filter_invalid_content(result)
            if filtered_result:
                valid_results.append(filtered_result)

        if not valid_results:
            return ""

        if len(valid_results) == 1:
            return valid_results[0]

        print(f"ğŸ§  å¼€å§‹æ™ºèƒ½ä¿¡æ¯æ•´åˆï¼Œå…± {len(valid_results)} ä¸ªæœ‰æ•ˆç»“æœ")

        current_level = valid_results.copy()
        level = 0

        while len(current_level) > 1:
            level += 1
            # Dynamic token allocation: increase tokens as we go up the merge tree
            base_tokens = 1000  # Base tokens for first level
            max_tokens = min(base_tokens + (level * 500), 4000)  # Cap at 4000 tokens

            print(
                f"  ğŸ“Š åˆå¹¶çº§åˆ« {level}ï¼Œå¤„ç† {len(current_level)} ä¸ªç‰‡æ®µï¼Œå…è®¸ {max_tokens} tokens"
            )

            # Create pairs for merging
            pairs = []
            for i in range(0, len(current_level), 2):
                if i + 1 < len(current_level):
                    pairs.append((current_level[i], current_level[i + 1]))
                else:
                    # Odd number: the last item goes to next level directly
                    pairs.append((current_level[i], ""))

            # Merge pairs in parallel using thread pool
            next_level = []
            with ThreadPoolExecutor(
                max_workers=min(self.max_workers, len(pairs)),
                thread_name_prefix="LI-merge_worker",
            ) as executor:
                future_to_pair = {}
                for idx, (content1, content2) in enumerate(pairs):
                    future = executor.submit(
                        self._merge_two_contents, content1, content2, max_tokens, level
                    )
                    future_to_pair[future] = idx

                # Collect results in order
                pair_results = [""] * len(pairs)
                for future in as_completed(future_to_pair):
                    pair_idx = future_to_pair[future]
                    try:
                        merged_result = future.result()
                        if merged_result:  # Only keep non-empty results
                            pair_results[pair_idx] = merged_result
                        print(
                            f"    âœ“ å®Œæˆåˆå¹¶å¯¹: {pair_idx}, {pair_idx + 1}. æ€»é•¿åº¦: {len(pairs)}"
                        )
                    except Exception as exc:
                        print(f"    âœ— åˆå¹¶å¯¹ {pair_idx}, {pair_idx + 1} å¤±è´¥: {exc}")
                        # Fallback: use the first content of the pair
                        pair_results[pair_idx] = (
                            pairs[pair_idx][0] if pairs[pair_idx][0] else ""
                        )

                # Filter out None and empty results
                next_level = [
                    result for result in pair_results if result and result.strip()
                ]

            if not next_level:
                # If all merging failed, return the best we have
                return valid_results[0] if valid_results else ""

            current_level = next_level
            print(f"  âœ… çº§åˆ« {level} å®Œæˆï¼Œå‰©ä½™ {len(current_level)} ä¸ªç‰‡æ®µ")

        final_result = current_level[0] if current_level else ""
        print(f"ğŸ¯ æ™ºèƒ½æ•´åˆå®Œæˆï¼Œæœ€ç»ˆç»“æœé•¿åº¦: {len(final_result)} å­—ç¬¦")

        return final_result

    # Generate a prompt-word paper abstract based on a single metadata
    def _process_single_paper(self, meta: Dict[str, Any]) -> None:
        """
        Process a single paper with error handling
        """
        try:
            arxiv_rate_limiter.wait_if_needed()
            raw_article_address = self.metadata_client.single_metadata_parser(meta)

            # Analyze the article
            ana_article = self.article_processor.analyze(
                self.pdf_parser.convert(raw_article_address).markdown_text
            )
            self.memory.add_memory(messages=ana_article, metadata={"id": meta["id"]})

            # Find connections
            self.result_queue.put(
                self.llm_embedding.find_connect(
                    article=ana_article, user_query=self.context.user_query
                )
            )
            self.context.successful_analyses += 1
            print(f"    âœ“ æˆåŠŸå¤„ç†: {meta['id']}")

        except Exception as exc:
            error_message = f"å¤„ç†å¤±è´¥ (ID: {meta['id']}): {exc}"
            self.result_queue.put(error_message)
            self.context.failed_analyses += 1
            print(f"    âœ— {error_message}")

    ### STATE FUNCTION
    # Startup Function
    def _handle_initialization(self) -> AgentState:
        """
        Initialize the agent and gather user input
        """
        print("ğŸ¤– æ™ºåº“ç´¢å¼•å·²å¯åŠ¨")

        user_query = self.interface.input("ç§‘ç ”äºº, ä»Šå¤©ä½ æ¥æ­¤åœ°æ˜¯ä¸ºäº†å¯»æ‰¾ä»€ä¹ˆ?")
        self.context.user_query = user_query

        # Logging
        self.context.add_execution_record(
            action=ActionType.QUERY_ANALYSIS,
            details={"user_query": user_query, "summary": "ç”¨æˆ·å·²å‘é€æ£€ç´¢è¯·æ±‚"},
        )

        return AgentState.ANALYZING_QUERY

    ### STATE FUNCTION
    # Keyword generation function
    def _handle_query_analysis(self) -> AgentState:
        """
        Analyze user query and generate initial keywords
        """
        print("ğŸ” åˆ†æç”¨æˆ·æŸ¥è¯¢ä¸­...")

        analysis_prompt = """
ä½ æ˜¯ä¸€ä¸ªç§‘ç ”æŸ¥è¯¢åˆ†æä¸“å®¶ã€‚åˆ†æç”¨æˆ·çš„ç ”ç©¶é—®é¢˜ï¼Œç”Ÿæˆé«˜è´¨é‡çš„æœç´¢å…³é”®è¯ã€‚

## è¦æ±‚ï¼š
    1. æå–æ ¸å¿ƒç ”ç©¶æ¦‚å¿µå’Œæ–¹æ³•
    2. è¯†åˆ«ç›¸å…³ç ”ç©¶é¢†åŸŸå’Œå­é¢†åŸŸ
    3. ç”Ÿæˆå¤šæ ·åŒ–çš„å…³é”®è¯ç»„åˆ
    4. è€ƒè™‘æŠ€æœ¯æœ¯è¯­å’Œé€šç”¨æœ¯è¯­çš„å¹³è¡¡
    5. è¾“å‡ºæ ¼å¼ï¼šä»…è¾“å‡ºå…³é”®è¯ï¼Œç”¨å¥å·åˆ†å‰²
"""

        message = [
            {"role": "system", "content": analysis_prompt},
            {"role": "user", "content": self.context.user_query},
        ]

        response = self.llm_query_processor.chat_completion(
            messages=message, temperature=0.7
        )

        initial_keywords = response["choices"][0]["message"]["content"]
        self.context.current_keywords = initial_keywords

        # Show generated keywords to user for feedback
        self.interface.output(
            f"è¿™äº›æ£€ç´¢å¯¹è±¡æ˜¯å¦è¶³å¤Ÿäº†? ä¸å¤Ÿè¯·ç»§ç»­è¡¥å……, æˆ‘ä¼šå¸¦ä¸Šä¸€èµ·æ‰¾: \n**{initial_keywords}**"
        )
        additional_keywords = self.interface.input(
            "(ç›´æ¥è¾“å…¥å…³é”®è¯, ç”¨é€—å·åˆ†å‰², æˆ–æŒ‰å›è½¦è·³è¿‡)"
        )

        if additional_keywords.strip():
            self.context.current_keywords += ", " + additional_keywords

        # Logging
        self.context.add_execution_record(
            action=ActionType.KEYWORD_GENERATION,
            details={
                "initial_keywords": initial_keywords,
                "additional_keywords": additional_keywords,
                "final_keywords": self.context.current_keywords,
                "summary": "å…³é”®è¯ç”Ÿæˆå®Œæˆ",
            },
        )

        return AgentState.PLANNING_SEARCH

    ### STATE FUNCTION
    # Function for accessing code generation
    def _handle_search_planning(self) -> AgentState:
        """
        Plan the search strategy based on current context
        """
        print("ğŸ“‹ è§„åˆ’æœç´¢ç­–ç•¥...")

        api_queries = self.api_rag.api_coding(self.context.current_keywords)

        search_plan = {
            "total_queries": len(api_queries),
            "max_papers_per_query": 2,
            "expected_total_papers": len(api_queries) * 2,
            "search_strategy": "systematic_parallel",
        }

        print(
            f"ğŸ“Š æœç´¢è®¡åˆ’: {search_plan['total_queries']} ä¸ªæŸ¥è¯¢èŠ‚ç‚¹, é¢„æœŸæ‰¾åˆ° {search_plan['expected_total_papers']} ç¯‡è®ºæ–‡"
        )

        # Logging
        self.context.add_execution_record(
            action=ActionType.SEARCH_EXECUTION,
            details={
                "api_queries": api_queries,
                "search_plan": search_plan,
                "summary": f"ç”Ÿæˆ {len(api_queries)} ä¸ªæœç´¢æŸ¥è¯¢",
            },
        )

        # Store queries in context for execution
        self.context.search_results = [
            {"query": query, "status": "pending"} for query in api_queries
        ]

        return AgentState.EXECUTING_SEARCH

    ### STATE FUNCTION
    # Function for searching through accessing code
    def _handle_search_execution(self) -> AgentState:
        """
        Execute the planned searches
        """
        print("ğŸ” æ‰§è¡Œæœç´¢ç­–ç•¥...")

        all_metadata: List[Dict[str, Any]] = []
        papers_found_in_attempt = False

        for i, search_item in enumerate(self.context.search_results):
            if search_item["status"] != "pending":
                continue

            query = search_item["query"]
            print(f"[{i+1}/{len(self.context.search_results)}] æ‰§è¡ŒæŸ¥è¯¢: {query}")

            # Strictly require access speed to be less than 1 time per 3 seconds on ARXIV_STANDARD
            arxiv_rate_limiter.wait_if_needed()

            try:
                metadata_list = self.metadata_client.search_get_metadata(
                    query=query, max_num=2
                )

                # Retrieve available results
                if metadata_list:
                    papers_found_in_attempt = True
                    all_metadata.extend(metadata_list)
                    search_item["status"] = "completed"
                    search_item["results"] = metadata_list
                    print(f"  âœ“ æ‰¾åˆ° {len(metadata_list)} ç¯‡è®ºæ–‡")
                # No available results
                else:
                    search_item["status"] = "no_results"
                    print(f"  âš  æ­¤æŸ¥è¯¢æœªæ‰¾åˆ°è®ºæ–‡")

            except Exception as exc:
                search_item["status"] = "error"
                search_item["error"] = str(exc)
                print(f"  âœ— æœç´¢å¤±è´¥: {exc}")

        # Logging
        self.context.total_papers_found = len(all_metadata)
        self.context.search_attempts += 1
        self.context.add_execution_record(
            action=ActionType.SEARCH_EXECUTION,
            details={
                "papers_found": len(all_metadata),
                "attempt_number": self.context.search_attempts,
                "summary": f"æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(all_metadata)} ç¯‡è®ºæ–‡",
            },
        )

        # Store metadata for processing
        self.all_metadata = all_metadata

        if papers_found_in_attempt:
            return AgentState.PROCESSING_RESULTS
        else:
            return AgentState.EVALUATING_RESULTS

    ### STATE FUNCTION
    # Structuring the paper into prompt words
    def _handle_result_processing(self) -> AgentState:
        """
        Process the found papers using LLM analysis
        """
        print("ğŸ§  å¤„ç†è®ºæ–‡å†…å®¹...")

        if not hasattr(self, "all_metadata") or not self.all_metadata:
            return AgentState.EVALUATING_RESULTS

        with ThreadPoolExecutor(
            max_workers=self.max_workers, thread_name_prefix="LI-llm_worker"
        ) as executor:
            futures = []

            for meta in self.all_metadata:
                print(f"  ğŸ“„ å¤„ç†è®ºæ–‡: {meta.get('id', 'unknown')}")

                # Check memory first
                cached_analysis = self.memory.search(meta["id"])
                if cached_analysis:
                    print("    âœ“ ä»è®°å¿†å±‚è·å–åˆ†æç»“æœ")
                    try:
                        result = self.llm_embedding.find_connect(
                            article=cached_analysis[0]["memory"],
                            user_query=self.context.user_query,
                        )
                        self.result_queue.put(result)
                        self.context.successful_analyses += 1
                    except Exception as exc:
                        self.result_queue.put(
                            f"è®°å¿†å±‚å¤„ç†é”™è¯¯ (ID: {meta['id']}): {exc}"
                        )
                        self.context.failed_analyses += 1

                # Direct parsing of non-indexed content in the memory layer
                else:
                    # Submit to process
                    future = executor.submit(self._process_single_paper, meta)
                    futures.append(future)

            # Wait for all processing to complete
            for future in as_completed(futures):
                try:
                    future.result()
                except Exception as exc:
                    print(f"  âœ— è®ºæ–‡å¤„ç†å¤±è´¥: {exc}")

        self.context.processed_papers = len(self.all_metadata)

        # Logging
        self.context.add_execution_record(
            action=ActionType.RESULT_PROCESSING,
            details={
                "total_processed": self.context.processed_papers,
                "successful": self.context.successful_analyses,
                "failed": self.context.failed_analyses,
                "summary": f"å¤„ç†å®Œæˆï¼š{self.context.successful_analyses}/{self.context.processed_papers} æˆåŠŸ",
            },
        )

        return AgentState.EVALUATING_RESULTS

    ### STATE FUNCTION
    # Evaluator for search results
    def _handle_result_evaluation(self) -> AgentState:
        """
        Evaluate the quality of results and decide next action
        """
        print("ğŸ“Š è¯„ä¼°æœç´¢ç»“æœè´¨é‡...")

        evaluation = self._evaluate_search_quality()

        print(f"  ğŸ“ˆ è¯„ä¼°ç»“æœï¼š")
        print(f"    - æ‰¾åˆ°è®ºæ–‡: {evaluation['papers_found']}")
        print(f"    - æˆåŠŸç‡: {evaluation['success_rate']:.2%}")
        print(f"    - æœç´¢æ•ˆç‡: {evaluation['search_efficiency']:.2f}")
        print(f"    - å»ºè®®è¡ŒåŠ¨: {evaluation['suggested_action']}")

        # Logging
        self.context.add_execution_record(
            action=ActionType.STRATEGY_REFINEMENT,
            details={
                "evaluation": evaluation,
                "summary": f"è¯„ä¼°å®Œæˆï¼Œå»ºè®®: {evaluation['suggested_action']}",
            },
        )

        if (
            evaluation["needs_refinement"]
            and self.context.search_attempts < self.max_search_retries
        ):
            return AgentState.REFINING_STRATEGY
        else:
            return AgentState.SYNTHESIZING

    ### STATE FUNCTION
    # Avoid generating the same keywords and regenerate based on it
    def _handle_strategy_refinement(self) -> AgentState:
        """
        Refine search strategy based on evaluation
        """
        print("ğŸ”§ ä¼˜åŒ–æœç´¢ç­–ç•¥...")

        evaluation = self._evaluate_search_quality()
        new_keywords = self._generate_adaptive_keywords(evaluation)

        print(f"  ğŸ”„ å…³é”®è¯ä¼˜åŒ–:")
        print(f"    åŸå…³é”®è¯: {self.context.current_keywords}")
        print(f"    æ–°å…³é”®è¯: {new_keywords}")

        self.context.current_keywords = new_keywords

        # Logging
        self.context.add_execution_record(
            action=ActionType.STRATEGY_REFINEMENT,
            details={
                "old_keywords": self.context.current_keywords,
                "new_keywords": new_keywords,
                "summary": "å®Œæˆæœç´¢ç­–ç•¥ä¼˜åŒ–",
            },
        )

        return AgentState.PLANNING_SEARCH

    ### STATE FUNCTION
    # Combining all the previous papers to generate the final results with intelligent synthesis
    def _handle_synthesis(self) -> AgentState:
        """
        Synthesize all results using intelligent binary-merge algorithm and present to user
        """
        print("ğŸ”¬ ç»¼åˆåˆ†æç»“æœ...")

        # Collect all results
        results = []
        while not self.result_queue.empty():
            results.append(self.result_queue.get())

        self.context.analysis_results = results

        if results:
            print(f"ğŸ“„ æ”¶é›†åˆ° {len(results)} ä¸ªåˆ†æç»“æœï¼Œå¼€å§‹æ•´åˆæ‰€æœ‰ä¿¡æ¯...")

            # Use intelligent synthesis merge instead of simple concatenation
            intelligently_merged_content = self._intelligent_synthesis_merge(results)

            synthesis_summary = f"""

# ğŸ¯ æ™ºåº“ç´¢å¼•æ‰§è¡ŒæŠ¥å‘Š

## ğŸ“‹ æ‰§è¡Œæ¦‚å†µ
- æŸ¥è¯¢åˆ†æ: âœ“
- æœç´¢å°è¯•: {self.context.search_attempts} æ¬¡
- æ‰¾åˆ°è®ºæ–‡: {self.context.total_papers_found} ç¯‡
- æˆåŠŸåˆ†æ: {self.context.successful_analyses} ç¯‡
- åˆ†ææˆåŠŸç‡: {(self.context.successful_analyses/max(1,self.context.processed_papers)):.1%}

## ğŸ“š ç ”ç©¶å‘ç°
{intelligently_merged_content}
"""
            print("âœ¨ å…¨æ•´åˆå®Œæˆ")
            print(synthesis_summary)
            self.interface.output(synthesis_summary)

        else:
            no_result_message = f"""
# ğŸ¯ æ™ºåº“ç´¢å¼•æ‰§è¡ŒæŠ¥å‘Š

ç»è¿‡ {self.context.search_attempts} æ¬¡æ™ºèƒ½æœç´¢å°è¯•ï¼Œæœªèƒ½æ‰¾åˆ°ä¸æ‚¨çš„æŸ¥è¯¢ç›´æ¥ç›¸å…³çš„é«˜è´¨é‡è®ºæ–‡ã€‚

## å»ºè®®
1. å°è¯•æ›´é€šç”¨æˆ–ç›¸å…³çš„å…³é”®è¯
2. æ‰©å±•æœç´¢åˆ°ç›¸å…³ç ”ç©¶é¢†åŸŸ
3. æ£€æŸ¥æŸ¥è¯¢çš„å…·ä½“æ€§æ˜¯å¦åˆé€‚
"""
            print(no_result_message)
            self.interface.output(no_result_message)

        # Logging
        self.context.add_execution_record(
            action=ActionType.SYNTHESIS,
            details={
                "total_results": len(results),
                "execution_summary": {
                    "search_attempts": self.context.search_attempts,
                    "papers_found": self.context.total_papers_found,
                    "successful_analyses": self.context.successful_analyses,
                },
                "summary": "å·²å®Œæˆè§£æä¸æç¤ºè¯ç´¢å¼•ç”Ÿæˆ",
            },
        )

        return AgentState.COMPLETED

    # Process execution function
    # The only directly callable function of this class
    def execute(self) -> str:
        """
        Main execution method with state-based planning.
        """
        try:
            while self.context.current_state not in [
                AgentState.COMPLETED,
                AgentState.ERROR,
            ]:
                current_state = self.context.current_state

                if current_state in self.state_handlers:
                    next_state = self.state_handlers[current_state]()
                    self._transition_state(next_state)
                else:
                    print(f"âš ï¸ æœªå®šä¹‰çŠ¶æ€: {current_state}")
                    self._transition_state(AgentState.ERROR)
                    break

            if self.context.current_state == AgentState.COMPLETED:
                print(f"âœ… æ™ºèƒ½ç ”ç©¶åŠ©æ‰‹æ‰§è¡Œå®Œæˆ")
                return (
                    "\n".join(self.context.analysis_results)
                    if self.context.analysis_results
                    else "æ‰§è¡Œå®Œæˆï¼Œä½†æœªæ‰¾åˆ°ç›¸å…³ç»“æœ"
                )
            else:
                print(f"âŒ è¿è¡Œæ—¶å¼‚å¸¸ç»ˆæ­¢")
                return "æ‰§è¡Œä¸­å‘ç”Ÿé”™è¯¯"

        except KeyboardInterrupt:
            print(f"âŒ æ‰§è¡Œè¢«æ‰‹åŠ¨ç»ˆæ­¢")
            return "ç”¨æˆ·æ‰‹åŠ¨ç»ˆæ­¢æ£€ç´¢"

        except Exception as exc:
            print(f"âŒ æ‰§è¡Œå¼‚å¸¸: {exc}")
            self._transition_state(AgentState.ERROR)
            return f"æ‰§è¡Œå¼‚å¸¸: {exc}"


def main(
    interface: str,
    raw_message_process_llm: str,
    raw_message_process_llm_model: str,
    api_generate_llm: str,
    api_generate_llm_model: str,
    embedding_llm: str,
    embedding_llm_model: str,
    max_workers_llm=8,
    max_search_retries=3,
) -> str:
    """
    Intelligent Research Agent - Advanced AI system with state-based planning

    params
    ------
    interface: str - Interface type ("debug", etc.)
    raw_message_process_llm: str - LLM provider for query processing
    raw_message_process_llm_model: str - Model for query processing
    api_generate_llm: str - LLM provider(RAG) for API generation
    api_generate_llm_model: str - Model for API generation
    embedding_llm: str - LLM provider for embedding and finding connections
    embedding_llm_model: str - Model for embedding and finding connections
    max_workers_llm: int - Maximum concurrent workers for LLM processing
    max_search_retries: int - Maximum search retry attempts

    return
    ------
    str - Final research results or execution summary
    """

    # Configuration for the intelligent agent
    agent_config = {
        "interface": interface,
        "raw_message_process_llm": raw_message_process_llm,
        "raw_message_process_llm_model": raw_message_process_llm_model,
        "api_generate_llm": api_generate_llm,
        "api_generate_llm_model": api_generate_llm_model,
        "embedding_llm": embedding_llm,
        "embedding_llm_model": embedding_llm_model,
        "max_workers_llm": max_workers_llm,
        "max_search_retries": max_search_retries,
    }

    # Create and execute the intelligent research agent
    agent = IntelligentResearchAgent(agent_config)
    return agent.execute()
